{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content page\n",
    "\n",
    "- [How to read an audio file?]()\n",
    "- [What is a sample rate, number of samples, waveform and spectrogram?]()\n",
    "- [What is overlapping and frame window?]()\n",
    "- [What is VAD? & other queries about VAD]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to read an audio file?\n",
    "\n",
    "**Qn: through python, how can I read a .wav file, get the sampling rate, number of samples and time data?**\n",
    "\n",
    "**Response:**\n",
    "You can use the wave module in Python to read a .wav file and extract information like the sampling rate, number of samples, and time data. Here's a step-by-step guide:\n",
    "\n",
    "\n",
    "1. Install the required module:\n",
    "```python\n",
    "    import wave\n",
    "    import numpy as np\n",
    "```\n",
    "\n",
    "2. Open the `.wav` file:\n",
    "```python\n",
    "    with wave.open('yourfile.wav', 'rb') as wav_file:       \n",
    "\n",
    "    sampling_rate = wav_file.getframerate() # Get the sampling rate                    \n",
    "        \n",
    "    num_samples = wav_file.getnframes() # Get the number of samples                  \n",
    "        \n",
    "    duration = num_samples / sampling_rate # Get the duration in seconds              \n",
    "        \n",
    "    frames = wav_file.readframes(num_samples) # Read the frames and convert to numpy array      \n",
    "    time_data = np.frombuffer(frames, dtype=np.int16)\n",
    "```\n",
    "\n",
    "3. Print the extracted information:\n",
    "```python\n",
    "    print(f\"Sampling Rate: {sampling_rate} Hz\")\n",
    "    print(f\"Number of Samples: {num_samples}\")\n",
    "    print(f\"Duration: {duration} seconds\")\n",
    "```\n",
    "\n",
    "This code will help you read the `.wav` file and extract the necessary information. If you have any questions or need further assistance, feel free to ask!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a sample rate, number of samples, waveform and spectrogram?\n",
    "\n",
    "**Qn: May I know what is sample rate and number of samples meaning?**\n",
    "\n",
    "**Response:** Absolutely! Here's a brief explanation:\n",
    "Sample Rate\n",
    "The sample rate, also known as the sampling frequency, is the number of samples of audio carried per second. It's measured in Hertz (Hz). For example, a sample rate of 44,100 Hz means that 44,100 samples are taken every second. Higher sample rates can capture more detail in the audio signal.\n",
    "\n",
    "Number of Samples\n",
    "The number of samples refers to the total count of individual audio data points in the file. Each sample represents the amplitude of the audio signal at a specific point in time. The total number of samples can be calculated by multiplying the sample rate by the duration of the audio file.\n",
    "\n",
    "Example\n",
    "If you have a .wav file with a sample rate of 44,100 Hz and a duration of 10 seconds, the number of samples would be:\n",
    "Number of Samples=Sample Rate×Duration=44,100×10=441,000\n",
    "\n",
    "Does this help clarify things? Feel free to ask if you have more questions!\n",
    "\n",
    "**Extra explanation found from YouTube:** https://www.youtube.com/watch?v=xJKypZXc3B4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qn: what is considered time data in an audio file?**\n",
    "\n",
    "**Response:** In an audio file, time data refers to the sequence of samples that represent the amplitude of the audio signal over time. Here's a breakdown:\n",
    "\n",
    "Time Data Components\n",
    "1. Samples: Each sample is a data point that represents the amplitude (volume) of the audio signal at a specific moment. These samples are taken at regular intervals determined by the sample rate. \n",
    "2. Time Axis: The time axis is essentially the timeline of the audio signal. Each sample corresponds to a specific point in time, and together, they form a continuous representation of the audio signal over the duration of the file. \n",
    "\n",
    "Visual Representation\n",
    "When you visualize an audio waveform, the horizontal axis represents time, and the vertical axis represents amplitude. The waveform is a graphical representation of the time data.\n",
    "\n",
    "Example\n",
    "If you have an audio file with a sample rate of 44,100 Hz and a duration of 1 second, you will have 44,100 samples. Each sample represents the amplitude of the audio signal at a specific point in time within that second.\n",
    "\n",
    "Usage\n",
    "Time data is crucial for various audio processing tasks, such as:\n",
    "- Editing: Cutting, copying, and pasting sections of audio. \n",
    "-  Analysis: Extracting features like pitch, tempo, and rhythm. \n",
    "- Synthesis: Generating new sounds or modifying existing ones.\n",
    "\n",
    "Does this help clarify what time data is in an audio file? Feel free to ask if you have more questions or need further details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qn: isnt the wave form of the audio a spectogram? or am I mistaken? How can I create a waveform in python given that I have a wave file?**\n",
    "\n",
    "**Response:** It's a common confusion! Let's clarify the difference:\n",
    "Waveform vs. Spectrogram\n",
    "- Waveform: This is a graphical representation of the audio signal in the time domain. It shows how the amplitude of the signal varies over time. \n",
    "- Spectrogram: This is a visual representation of the spectrum of frequencies in a signal as they vary with time. It shows how the frequency content of the signal changes over time. \n",
    "\n",
    "Creating a Waveform in Python\n",
    "\n",
    "To create a waveform from a .wav file in Python, you can use libraries like wave and matplotlib. Here's an example:\n",
    "1. Install the required libraries: \n",
    "```python\n",
    "    pip install numpy matplotlib\n",
    "```\n",
    "\n",
    "2. Read the .wav file and plot the waveform: \n",
    "```python\n",
    "   import wave\n",
    "   import numpy as np\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   # Open the .wav file\n",
    "   with wave.open('yourfile.wav', 'rb') as wav_file:\n",
    "       # Get the number of frames\n",
    "       num_frames = wav_file.getnframes()\n",
    "\n",
    "       # Read the frames\n",
    "       frames = wav_file.readframes(num_frames)\n",
    "\n",
    "       # Convert frames to numpy array\n",
    "       waveform = np.frombuffer(frames, dtype=np.int16)\n",
    "\n",
    "       # Get the sample rate\n",
    "       sample_rate = wav_file.getframerate()\n",
    "\n",
    "       # Create a time array\n",
    "       time = np.linspace(0, num_frames / sample_rate, num=num_frames)\n",
    "\n",
    "   # Plot the waveform\n",
    "   plt.figure(figsize=(10, 4))\n",
    "   plt.plot(time, waveform)\n",
    "   plt.title('Waveform')\n",
    "   plt.xlabel('Time [s]')\n",
    "   plt.ylabel('Amplitude')\n",
    "   plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "This code will read the .wav file, extract the waveform data, and plot it using matplotlib.\n",
    "Would you like to know how to create a spectrogram as well?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qn: why does my number of samples and time not match?**\n",
    "```shell\n",
    "ValueError Traceback (most recent call last)\n",
    "Cell In[31], line 2\n",
    "1 clip_2 = 'clip2.wav' \n",
    "----> 2 sampling_rate, num_samples, duration, time_data = extract_info(clip_2)\n",
    "\n",
    "Cell In[29], line 23, in extract_info(audio_clip)\n",
    "20 time = np.arange(num_samples) / sampling_rate\n",
    "22 print_info(sampling_rate, num_samples, duration)\n",
    "---> 23 plot_waveform(time_data, time)\n",
    "25 return sampling_rate, num_samples, duration, time_data\n",
    "\n",
    "Cell In[29], line 36, in plot_waveform(waveform, time)\n",
    "34 def plot_waveform(waveform, time):\n",
    "35 plt.figure(figsize=(10,4))\n",
    "---> 36 plt.plot(time, waveform)\n",
    "37 plt.title(\"Wavefrom\")\n",
    "38 plt.xlabel(\"Time(s)\")\n",
    "\n",
    "File ~\\AppData\\Roaming\\Python\\Python313\\site-packages\\matplotlib\\pyplot.py:3827, in plot(scalex, scaley, data, args, *kwargs)\n",
    "3819 @_copy_docstring_and_deprecators(Axes.plot)\n",
    "3820 def plot(\n",
    "3821 *args: float | ArrayLike | str,\n",
    "(...) 3825 **kwargs,\n",
    "3826 ) -> list[Line2D]:\n",
    "...\n",
    "496 if x.ndim > 2 or y.ndim > 2:\n",
    "497 raise ValueError(f\"x and y can be no greater than 2D, but have \"\n",
    "498 f\"shapes {x.shape} and {y.shape}\")\n",
    "\n",
    "ValueError: x and y must have same first dimension, but have shapes (144144,) and (288288,)\n",
    "\n",
    "Given that this is the audio information:\n",
    "Sampling Rate: 32000 Hz\n",
    "Number of Samples: 144144\n",
    "Duration: 4.5045 seconds\n",
    "\n",
    "```\n",
    "\n",
    "**Response:** The error you're encountering suggests that the time_data array has twice the number of elements as the time array. This discrepancy is likely due to the audio file being stereo (two channels), while the time array is created for a mono (single channel) signal.\n",
    "\n",
    "Solution\n",
    "To handle stereo audio, you need to separate the channels and ensure the time array matches the length of one channel. Here's how you can modify your code:\n",
    "\n",
    "1. Check the number of channels: \n",
    "```python\n",
    "   import wave\n",
    "   import numpy as np\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   def extract_info(audio_clip):\n",
    "       with wave.open(audio_clip, 'rb') as wav_file:\n",
    "           sampling_rate = wav_file.getframerate()\n",
    "           num_samples = wav_file.getnframes()\n",
    "           num_channels = wav_file.getnchannels()\n",
    "           duration = num_samples / sampling_rate\n",
    "\n",
    "           # Convert file to numpy array\n",
    "           frames = wav_file.readframes(num_samples)\n",
    "           time_data = np.frombuffer(frames, dtype=np.int16)\n",
    "\n",
    "           # If stereo, reshape the array to separate channels\n",
    "           if num_channels == 2:\n",
    "               time_data = time_data.reshape(-1, 2)\n",
    "               time_data = time_data[:, 0]  # Use only the first channel for plotting\n",
    "\n",
    "           # Create a time array\n",
    "           time = np.arange(len(time_data)) / sampling_rate\n",
    "\n",
    "           print_info(sampling_rate, num_samples, duration)\n",
    "           plot_waveform(time_data, time)\n",
    "\n",
    "       return sampling_rate, num_samples, duration, time_data\n",
    "\n",
    "   def print_info(sampling_rate, num_samples, duration):\n",
    "       print(f\"Sampling Rate: {sampling_rate} Hz\")\n",
    "       print(f\"Number of Samples: {num_samples}\")\n",
    "       print(f\"Duration: {duration} seconds\")\n",
    "\n",
    "   def plot_waveform(waveform, time):\n",
    "       plt.figure(figsize=(10, 4))\n",
    "       plt.plot(time, waveform)\n",
    "       plt.title(\"Waveform\")\n",
    "       plt.xlabel(\"Time (s)\")\n",
    "       plt.ylabel(\"Amplitude (dB)\")\n",
    "       plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "Key Adjustments:\n",
    "- Check the number of channels: num_channels = wav_file.getnchannels() \n",
    "- Reshape the array if stereo: time_data = time_data.reshape(-1, 2) \n",
    "- Use only one channel for plotting: time_data = time_data[:, 0] \n",
    "\n",
    "\n",
    "This should ensure that the time and time_data arrays have matching dimensions. Give this a try and let me know if it resolves the issue! If you encounter any other problems, feel free to ask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qn: How can I plot the spectogram from my extracted information function?**\n",
    "\n",
    "**Response:** To plot a spectrogram from your extracted audio information, you can use the librosa library, which provides a convenient way to compute and visualize spectrograms. Here's how you can modify your code to include spectrogram plotting:\n",
    "\n",
    "1. Install librosa: \n",
    "```python\n",
    "   python -m pip install librosa\n",
    "```\n",
    "\n",
    "2. Modify your code to include spectrogram plotting: \n",
    "```python\n",
    "      import wave\n",
    "   import numpy as np\n",
    "   import matplotlib.pyplot as plt\n",
    "   import librosa\n",
    "   import librosa.display\n",
    "\n",
    "   def extract_info(audio_clip):\n",
    "       with wave.open(audio_clip, 'rb') as wav_file:\n",
    "           sampling_rate = wav_file.getframerate()\n",
    "           num_samples = wav_file.getnframes()\n",
    "           duration = num_samples / sampling_rate\n",
    "\n",
    "           # Check the number of channels\n",
    "           num_channels = wav_file.getnchannels()\n",
    "\n",
    "           # Convert file to numpy array\n",
    "           frames = wav_file.readframes(num_samples)\n",
    "           time_data = np.frombuffer(frames, dtype=np.int16)\n",
    "\n",
    "           # If stereo, reshape the array to separate channels\n",
    "           if num_channels == 2:\n",
    "               time_data = time_data.reshape(-1, 2)\n",
    "               time_data = time_data[:, 0]  # Use only the first channel for plotting\n",
    "\n",
    "           # Create a time array\n",
    "           time = np.linspace(0, duration, num=num_samples)\n",
    "\n",
    "           print_info(sampling_rate, num_samples, duration)\n",
    "           plot_waveform(time_data, time)\n",
    "           plot_spectrogram(time_data, sampling_rate)\n",
    "\n",
    "       return sampling_rate, num_samples, duration, time_data\n",
    "\n",
    "   def print_info(sampling_rate, num_samples, duration):\n",
    "       print(f\"Sampling Rate: {sampling_rate} Hz\")\n",
    "       print(f\"Number of Samples: {num_samples}\")\n",
    "       print(f\"Duration: {duration} seconds\")\n",
    "\n",
    "   def plot_waveform(waveform, time):\n",
    "       plt.figure(figsize=(10, 4))\n",
    "       plt.plot(time, waveform)\n",
    "       plt.title(\"Waveform\")\n",
    "       plt.xlabel(\"Time (s)\")\n",
    "       plt.ylabel(\"Amplitude (dB)\")\n",
    "       plt.show()\n",
    "\n",
    "   def plot_spectrogram(waveform, sampling_rate):\n",
    "       # Compute the spectrogram\n",
    "       S = librosa.stft(waveform)\n",
    "       S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "\n",
    "       # Plot the spectrogram\n",
    "       plt.figure(figsize=(10, 4))\n",
    "       librosa.display.specshow(S_db, sr=sampling_rate, x_axis='time', y_axis='log')\n",
    "       plt.colorbar(format='%+2.0f dB')\n",
    "       plt.title('Spectrogram')\n",
    "       plt.xlabel('Time (s)')\n",
    "       plt.ylabel('Frequency (Hz)')\n",
    "       plt.show()\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "- librosa.stft: Computes the Short-Time Fourier Transform (STFT) of the audio signal, which is used to create the spectrogram. \n",
    "- librosa.amplitude_to_db: Converts the amplitude spectrogram to a decibel (dB) scale. \n",
    "- librosa.display.specshow: Plots the spectrogram. \n",
    "\n",
    "\n",
    "This code will plot both the waveform and the spectrogram of your audio file. Give it a try and let me know if it works! If you encounter any issues or have further questions, feel free to ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is overlapping and frame window?\n",
    "\n",
    "**Qn: I was given this task: Process the signal in frames with 50% overlap, print the number of frames to be processed. What does it mean by overlapping the signal frames**\n",
    "\n",
    "**Response:** Overlapping signal frames means dividing the audio signal into smaller segments (frames) where each frame overlaps with the previous one by a certain percentage. In your case, a 50% overlap means that each frame shares half of its samples with the previous frame.\n",
    "\n",
    "\n",
    "Why Overlap Frames?\n",
    "Overlapping frames is a common technique in signal processing to ensure smooth transitions between frames and to capture more detailed information about the signal. It helps in reducing artifacts and improving the accuracy of analysis, especially in tasks like speech processing and feature extraction.\n",
    "\n",
    "\n",
    "How to Calculate the Number of Frames with 50% Overlap\n",
    "1. Frame Size: The number of samples in each frame. \n",
    "2. Hop Size: The number of samples to move forward for the next frame. With 50% overlap, the hop size is half the frame size. \n",
    "\n",
    "Example Calculation\n",
    "Let's say you have an audio signal with:\n",
    "- Total Samples: ( N ) \n",
    "- Frame Size: ( F ) \n",
    "- Hop Size: ( H = \\frac{F}{2} ) \n",
    "The number of frames ( K ) can be calculated as: [ K = \\left\\lceil \\frac{N - F}{H} \\right\\rceil + 1 ]\n",
    "\n",
    "Python Code Example\n",
    "Here's how you can implement this in Python:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def process_frames(signal, frame_size, overlap):\n",
    "    hop_size = int(frame_size * (1 - overlap))\n",
    "    num_samples = len(signal)\n",
    "    num_frames = int(np.ceil((num_samples - frame_size) / hop_size)) + 1\n",
    "\n",
    "    print(f\"Number of frames to be processed: {num_frames}\")\n",
    "\n",
    "    frames = []\n",
    "    for i in range(num_frames):\n",
    "        start = i * hop_size\n",
    "        end = start + frame_size\n",
    "        frame = signal[start:end]\n",
    "        frames.append(frame)\n",
    "\n",
    "    return frames\n",
    "\n",
    "# Example usage\n",
    "signal = np.random.randn(10000)  # Example signal with 10,000 samples\n",
    "frame_size = 1024\n",
    "overlap = 0.5\n",
    "\n",
    "frames = process_frames(signal, frame_size, overlap)\n",
    "```\n",
    "Explanation:\n",
    "- hop_size = int(frame_size * (1 - overlap)): Calculates the hop size based on the frame size and overlap percentage. \n",
    "- num_frames = int(np.ceil((num_samples - frame_size) / hop_size)) + 1: Calculates the number of frames. \n",
    "- Loop: Iterates through the signal to extract overlapping frames. \n",
    "\n",
    "This code will process the signal in frames with 50% overlap and print the number of frames to be processed. Give it a try and let me know if you have any questions or need further assistance!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qn: can I make the graph of the overlap region? is it a waveform graph that I would be plotting?**\n",
    "\n",
    "**Response:** Yes, you can visualize the overlap region by plotting the waveform of the overlapping frames. This will help you see how the frames overlap and how the signal transitions between them.\n",
    "\n",
    "Steps to Plot Overlapping Frames\n",
    "1. Extract overlapping frames: Use the process_frames function to get the frames with overlap. \n",
    "2. Plot the waveform of the overlapping frames: Visualize the frames to see the overlap. \n",
    "\n",
    "Example Code\n",
    "Here's how you can modify your code to plot the waveform of the overlapping frames:\n",
    "```python\n",
    "import wave\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_info(audio_clip):\n",
    "    with wave.open(audio_clip, 'rb') as wav_file:\n",
    "        sampling_rate = wav_file.getframerate()\n",
    "        num_samples = wav_file.getnframes()\n",
    "        duration = num_samples / sampling_rate\n",
    "\n",
    "        print_info(sampling_rate, num_samples, duration)\n",
    "\n",
    "        # Create a time array as index --> use for spectrogram and waveform\n",
    "        time_index = np.linspace(0, duration, num=num_samples)\n",
    "\n",
    "        # Check the number of channels\n",
    "        num_channels = wav_file.getnchannels()\n",
    "\n",
    "        # Convert file to numpy array\n",
    "        frames = wav_file.readframes(num_samples)\n",
    "        frames_data = np.frombuffer(frames, dtype=np.int16)\n",
    "\n",
    "        # If stereo, reshape the array to separate channels\n",
    "        if num_channels == 2:\n",
    "            frames_data = frames_data.reshape(-1, 2)\n",
    "            frames_data = frames_data[:, 0]  # Use only the first channel for processing\n",
    "\n",
    "        # Process frames with 50% overlap\n",
    "        frame_size = 1024  # Define your frame size\n",
    "        overlap = 0.5\n",
    "        frames = process_frames(frames_data, frame_size, overlap)\n",
    "\n",
    "        # Plot the overlapping frames\n",
    "        plot_overlapping_frames(frames, sampling_rate, frame_size, overlap)\n",
    "\n",
    "    return sampling_rate, num_samples, duration, frames_data\n",
    "\n",
    "def print_info(sampling_rate, num_samples, duration):\n",
    "    print(f\"Sampling Rate: {sampling_rate} Hz\")\n",
    "    print(f\"Number of Samples: {num_samples}\")\n",
    "    print(f\"Duration: {duration} seconds\")\n",
    "\n",
    "def process_frames(signal, frame_size, overlap):\n",
    "    hop_size = int(frame_size * (1 - overlap))\n",
    "    num_samples = len(signal)\n",
    "    num_frames = int(np.ceil((num_samples - frame_size) / hop_size)) + 1\n",
    "\n",
    "    print(f\"Number of frames to be processed: {num_frames}\")\n",
    "\n",
    "    frames = []\n",
    "    for i in range(num_frames):\n",
    "        start = i * hop_size\n",
    "        end = start + frame_size\n",
    "        frame = signal[start:end]\n",
    "        frames.append(frame)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def plot_overlapping_frames(frames, sampling_rate, frame_size, overlap):\n",
    "    hop_size = int(frame_size * (1 - overlap))\n",
    "    time = np.arange(len(frames[0])) / sampling_rate\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i, frame in enumerate(frames[:5]):  # Plot the first 5 frames for clarity\n",
    "        plt.plot(time + i * hop_size / sampling_rate, frame, label=f'Frame {i+1}')\n",
    "\n",
    "    plt.title(\"Overlapping Frames\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude (dB)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "clip_1 = 'clip1.wav'\n",
    "sampling_rate, num_samples, duration, frames_data = extract_info(clip_1)\n",
    "```\n",
    "Explanation:\n",
    "- plot_overlapping_frames Function: Plots the first few overlapping frames to visualize the overlap. \n",
    "- time + i * hop_size / sampling_rate: Adjusts the time axis for each frame to show the overlap. \n",
    "\n",
    "This code will help you visualize the overlapping frames and see how the signal transitions between them. Give it a try and let me know if you have any questions or need further assistance!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qn: Why is the frame_size or window size normally 1024? and what is the pros and cons for making the overlap larger or smaller?**\n",
    "\n",
    "**Response:** Why is the Frame Size Normally 1024?\n",
    "The frame size (or window size) of 1024 samples is commonly used in audio processing because it strikes a good balance between time and frequency resolution. Here’s why:\n",
    "1. Frequency Resolution: A larger frame size provides better frequency resolution, allowing you to distinguish between closely spaced frequencies. For example, with a sample rate of 44.1 kHz, a frame size of 1024 samples gives a frequency resolution of approximately 43 Hz. \n",
    "2. Time Resolution: A smaller frame size provides better time resolution, allowing you to capture rapid changes in the signal. However, too small a frame size can result in poor frequency resolution. \n",
    "3. Computational Efficiency: Frame sizes that are powers of two (like 1024) are computationally efficient for Fast Fourier Transform (FFT) algorithms, which are commonly used in audio processing. \n",
    "\n",
    "\n",
    "Pros and Cons of Larger Overlap\n",
    "Larger Overlap (e.g., 75%):\n",
    "- Pros:\n",
    "    - Better Temporal Continuity: Provides smoother transitions between frames, which is beneficial for tasks like speech processing and pitch detection. \n",
    "    - Improved Accuracy: Reduces artifacts and improves the accuracy of feature extraction and signal analysis. \n",
    "- Cons:\n",
    "    - Increased Computational Load: Requires more processing power and memory, as more frames need to be processed. \n",
    "    - Redundancy: Higher overlap increases redundancy, which might not be necessary for all applications [ref](https://dsp.stackexchange.com/questions/22175/how-should-i-select-window-size-and-overlaphttps://books.mercity.ai/books/Audio-Analysis-and-Synthesis---Introduction-to-Audio-Signal-Processing/audio_analysis_techniques/01_ShortTime_Fourier_Transform_and_Spectrograms). \n",
    "\n",
    "\n",
    "Pros and Cons of Smaller Overlap\n",
    "Smaller Overlap (e.g., 25%):\n",
    "- Pros:\n",
    "    - Reduced Computational Load: Requires less processing power and memory, making it more efficient. \n",
    "    - Faster Processing: Suitable for real-time applications where speed is crucial. \n",
    "- Cons:\n",
    "    - Lower Temporal Continuity: Can result in less smooth transitions between frames, potentially introducing artifacts. \n",
    "    - Reduced Accuracy: May not capture rapid changes in the signal as effectively [ref](https://dsp.stackexchange.com/questions/22175/how-should-i-select-window-size-and-overlaphttps://books.mercity.ai/books/Audio-Analysis-and-Synthesis---Introduction-to-Audio-Signal-Processing/audio_analysis_techniques/01_ShortTime_Fourier_Transform_and_Spectrograms). \n",
    "\n",
    "Choosing the Right Overlap\n",
    "The choice of overlap depends on the specific requirements of your application. For example:\n",
    "- Speech Processing: A larger overlap (e.g., 50-75%) is often preferred to ensure smooth transitions and accurate feature extraction. \n",
    "- Real-Time Processing: A smaller overlap (e.g., 25%) might be chosen to reduce computational load and ensure faster processing. \n",
    "\n",
    "Example Code to Visualize Overlapping Frames\n",
    "Here’s how you can visualize overlapping frames in your signal:\n",
    "```python\n",
    "import wave\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_info(audio_clip):\n",
    "    with wave.open(audio_clip, 'rb') as wav_file:\n",
    "        sampling_rate = wav_file.getframerate()\n",
    "        num_samples = wav_file.getnframes()\n",
    "        duration = num_samples / sampling_rate\n",
    "\n",
    "        print_info(sampling_rate, num_samples, duration)\n",
    "\n",
    "        # Create a time array as index --> use for spectrogram and waveform\n",
    "        time_index = np.linspace(0, duration, num=num_samples)\n",
    "\n",
    "        # Check the number of channels\n",
    "        num_channels = wav_file.getnchannels()\n",
    "\n",
    "        # Convert file to numpy array\n",
    "        frames = wav_file.readframes(num_samples)\n",
    "        frames_data = np.frombuffer(frames, dtype=np.int16)\n",
    "\n",
    "        # If stereo, reshape the array to separate channels\n",
    "        if num_channels == 2:\n",
    "            frames_data = frames_data.reshape(-1, 2)\n",
    "            frames_data = frames_data[:, 0]  # Use only the first channel for processing\n",
    "\n",
    "        # Process frames with 50% overlap\n",
    "        frame_size = 1024  # Define your frame size\n",
    "        overlap = 0.5\n",
    "        frames = process_frames(frames_data, frame_size, overlap)\n",
    "\n",
    "        # Plot the overlapping frames\n",
    "        plot_overlapping_frames(frames, sampling_rate, frame_size, overlap)\n",
    "\n",
    "    return sampling_rate, num_samples, duration, frames_data\n",
    "\n",
    "def print_info(sampling_rate, num_samples, duration):\n",
    "    print(f\"Sampling Rate: {sampling_rate} Hz\")\n",
    "    print(f\"Number of Samples: {num_samples}\")\n",
    "    print(f\"Duration: {duration} seconds\")\n",
    "\n",
    "def process_frames(signal, frame_size, overlap):\n",
    "    hop_size = int(frame_size * (1 - overlap))\n",
    "    num_samples = len(signal)\n",
    "    num_frames = int(np.ceil((num_samples - frame_size) / hop_size)) + 1\n",
    "\n",
    "    print(f\"Number of frames to be processed: {num_frames}\")\n",
    "\n",
    "    frames = []\n",
    "    for i in range(num_frames):\n",
    "        start = i * hop_size\n",
    "        end = start + frame_size\n",
    "        frame = signal[start:end]\n",
    "        frames.append(frame)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def plot_overlapping_frames(frames, sampling_rate, frame_size, overlap):\n",
    "    hop_size = int(frame_size * (1 - overlap))\n",
    "    time = np.arange(len(frames[0])) / sampling_rate\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i, frame in enumerate(frames[:5]):  # Plot the first 5 frames for clarity\n",
    "        plt.plot(time + i * hop_size / sampling_rate, frame, label=f'Frame {i+1}')\n",
    "\n",
    "    plt.title(\"Overlapping Frames\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude (dB)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "clip_1 = 'clip1.wav'\n",
    "sampling_rate, num_samples, duration, frames_data = extract_info(clip_1)\n",
    "```\n",
    "This code will help you visualize the overlapping frames and understand the impact of different overlap sizes on your signal processing tasks. Let me know if you have any more questions or need further assistance!\n",
    "\n",
    "**More references:**\n",
    "[1](https://dsp.stackexchange.com/questions/22175/how-should-i-select-window-size-and-overlap)\n",
    "[2](https://books.mercity.ai/books/Audio-Analysis-and-Synthesis---Introduction-to-Audio-Signal-Processing/audio_analysis_techniques/01_ShortTime_Fourier_Transform_and_Spectrograms)\n",
    "[3](https://sicorps.com/linux/understanding-sample-rate-and-frame-size-in-digital-audio/)\n",
    "[4](https://developer.mozilla.org/en-US/docs/Web/Media/Guides/Formats/Audio_concepts)\n",
    "[5](https://stackoverflow.com/questions/55113331/ffmpeg-confused-with-the-concept-of-audio-frame-size-and-its-calculation)\n",
    "[6](https://forum.audacityteam.org/t/overlay-multiple-frequency-analyses/38521)\n",
    "[7](https://www.reddit.com/r/embedded/comments/w66awf/whats_your_preferred_microcontroller_for_realtime/)\n",
    "[8](https://www.production-expert.com/production-expert-1/pro-tools-audiosuite-the-pro-and-cons)\n",
    "[9](https://www.dropbox.com/s/aq1aigdx8dq306q/Screenshot%202015-03-18%2017.38.20.png?dl=0)\n",
    "[10](https://www.dropbox.com/s/knvwqwg7s3mcz56/example.wav?dl=0)\n",
    "\n",
    "**Extra explanation found from YouTube:** https://www.youtube.com/watch?v=tgH-a-vaiq8, https://www.youtube.com/watch?v=_cBinHSssVk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is VAD? & other queries about VAD\n",
    "\n",
    "**Qn: I was given this task: Plot VAD outputs over the amplitude plots with x‐axis should be labeled in Time (s)What does VAD outputs mean over the amplitude plots?**\n",
    "\n",
    "**Response:** Voice Activity Detection (VAD) is a technique used to identify whether human speech is present in an audio signal. VAD outputs typically indicate the presence or absence of speech over time. When plotting VAD outputs over amplitude plots, you are essentially overlaying the detected speech segments on the waveform of the audio signal.\n",
    "\n",
    "Steps to Plot VAD Outputs Over Amplitude Plots\n",
    "1. Extract the audio signal and compute the VAD outputs. \n",
    "2. Plot the waveform of the audio signal. \n",
    "3. Overlay the VAD outputs on the waveform. \n",
    "\n",
    "Example Code\n",
    "Here's an example of how you can achieve this using Python:\n",
    "1. Install the required libraries: \n",
    "```python\n",
    "   python -m pip install numpy matplotlib webrtcvad\n",
    "```\n",
    "\n",
    "2. Code to extract VAD outputs and plot them: \n",
    "```python\n",
    "   import wave\n",
    "   import numpy as np\n",
    "   import matplotlib.pyplot as plt\n",
    "   import webrtcvad\n",
    "\n",
    "   def extract_info(audio_clip):\n",
    "       with wave.open(audio_clip, 'rb') as wav_file:\n",
    "           sampling_rate = wav_file.getframerate()\n",
    "           num_samples = wav_file.getnframes()\n",
    "           duration = num_samples / sampling_rate\n",
    "\n",
    "           print_info(sampling_rate, num_samples, duration)\n",
    "\n",
    "           # Create a time array as index --> use for spectrogram and waveform\n",
    "           time_index = np.linspace(0, duration, num=num_samples)\n",
    "\n",
    "           # Check the number of channels\n",
    "           num_channels = wav_file.getnchannels()\n",
    "\n",
    "           # Convert file to numpy array\n",
    "           frames = wav_file.readframes(num_samples)\n",
    "           frames_data = np.frombuffer(frames, dtype=np.int16)\n",
    "\n",
    "           # If stereo, reshape the array to separate channels\n",
    "           if num_channels == 2:\n",
    "               frames_data = frames_data.reshape(-1, 2)\n",
    "               frames_data = frames_data[:, 0]  # Use only the first channel for processing\n",
    "\n",
    "           # Convert to floating-point format\n",
    "           frames_data = frames_data.astype(np.float32)\n",
    "\n",
    "           # Process VAD\n",
    "           vad = webrtcvad.Vad()\n",
    "           vad.set_mode(1)  # Set aggressiveness mode (0-3)\n",
    "           frame_duration = 0.02  # 20 ms\n",
    "           frame_size = int(sampling_rate * frame_duration)\n",
    "           num_frames = len(frames_data) // frame_size\n",
    "\n",
    "           vad_outputs = []\n",
    "           for i in range(num_frames):\n",
    "               start = i * frame_size\n",
    "               end = start + frame_size\n",
    "               frame = frames_data[start:end].tobytes()\n",
    "               is_speech = vad.is_speech(frame, sample_rate=sampling_rate)\n",
    "               vad_outputs.append(is_speech)\n",
    "\n",
    "           # Plot the waveform and VAD outputs\n",
    "           plot_waveform_with_vad(frames_data, time_index, vad_outputs, frame_duration)\n",
    "\n",
    "       return sampling_rate, num_samples, duration, frames_data\n",
    "\n",
    "   def print_info(sampling_rate, num_samples, duration):\n",
    "       print(f\"Sampling Rate: {sampling_rate} Hz\")\n",
    "       print(f\"Number of Samples: {num_samples}\")\n",
    "       print(f\"Duration: {duration} seconds\")\n",
    "\n",
    "   def plot_waveform_with_vad(waveform, time, vad_outputs, frame_duration):\n",
    "       plt.figure(figsize=(10, 4))\n",
    "       plt.plot(time, waveform, label='Waveform')\n",
    "\n",
    "       # Overlay VAD outputs\n",
    "       for i, is_speech in enumerate(vad_outputs):\n",
    "           if is_speech:\n",
    "               start_time = i * frame_duration\n",
    "               end_time = start_time + frame_duration\n",
    "               plt.axvspan(start_time, end_time, color='red', alpha=0.3, label='Speech' if i == 0 else \"\")\n",
    "\n",
    "       plt.title(\"Waveform with VAD Outputs\")\n",
    "       plt.xlabel(\"Time (s)\")\n",
    "       plt.ylabel(\"Amplitude\")\n",
    "       plt.legend()\n",
    "       plt.show()\n",
    "\n",
    "   # Example usage\n",
    "   clip_1 = 'clip1.wav'\n",
    "   sampling_rate, num_samples, duration, frames_data = extract_info(clip_1)\n",
    "```\n",
    "Explanation:\n",
    "- webrtcvad: A library for Voice Activity Detection. \n",
    "- vad.is_speech: Determines if a frame contains speech. \n",
    "- plt.axvspan: Highlights the speech segments on the waveform plot.\n",
    "\n",
    "This code will help you visualize the VAD outputs over the amplitude plots, with the x-axis labeled in time (seconds). Give it a try and let me know if you have any questions or need further assistance!\n",
    "\n",
    "**Reference:**\n",
    "[1](https://www.mathworks.com/matlabcentral/answers/269391-how-to-plot-bode-diagram-from-input-and-output-data)\n",
    "[2](https://www.mathworks.com/help/slcontrol/ug/visualize-bode-response-of-simulink-model-during-simulation.html)\n",
    "[3](https://math.mit.edu/~stoopn/18.031/class2-reading.pdf)\n",
    "[4](https://en.wikipedia.org/wiki/Voice_activity_detection)\n",
    "[5](https://blog.milvus.io/ai-quick-reference/what-is-voice-activity-detection-vad-and-why-is-it-important)\n",
    "[6](https://speechprocessingbook.aalto.fi/Recognition/Voice_activity_detection.html)\n",
    "[7](https://www.tavus.io/post/voice-activity-detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bing Search: how to do detect voice activity in python**\n",
    "\n",
    "**Response:** Detecting voice activity in Python can be achieved using various libraries. Here are three approaches you can consider:\n",
    "\n",
    "1. Using pydub and numpy\n",
    "```python\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "\n",
    "def detect_voice_activity(audio_file):\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    samples = np.array(audio.get_array_of_samples())\n",
    "    threshold = np.mean(np.abs(samples)) * 1.5  # Adjust threshold as needed\n",
    "\n",
    "    voice_activity = np.where(np.abs(samples) > threshold, 1, 0)\n",
    "    return voice_activity\n",
    "\n",
    "audio_file = \"path/to/your/audiofile.wav\"\n",
    "activity = detect_voice_activity(audio_file)\n",
    "print(activity)\n",
    "```\n",
    "\n",
    "2. Using webrtcvad\n",
    "```python\n",
    "import webrtcvad\n",
    "import wave\n",
    "\n",
    "def read_wave(path):\n",
    "    with wave.open(path, 'rb') as wf:\n",
    "        num_channels = wf.getnchannels()\n",
    "        sample_width = wf.getsampwidth()\n",
    "        sample_rate = wf.getframerate()\n",
    "        pcm_data = wf.readframes(wf.getnframes())\n",
    "    return pcm_data, sample_rate\n",
    "\n",
    "def detect_voice_activity(audio_file):\n",
    "    vad = webrtcvad.Vad()\n",
    "    vad.set_mode(1) # Aggressiveness mode (0-3)</span>\n",
    "    \n",
    "    pcm_data, sample_rate = read_wave(audio_file)\n",
    "    frame_duration = 30  # ms\n",
    "    frame_size = int(sample_rate * frame_duration / 1000)\n",
    "    \n",
    "    voice_activity = []\n",
    "    for i in range(0, len(pcm_data), frame_size):\n",
    "        frame = pcm_data[i:i + frame_size]\n",
    "        is_speech = vad.is_speech(frame, sample_rate)\n",
    "        voice_activity.append(is_speech)\n",
    "    \n",
    "    return voice_activity\n",
    "\n",
    "audio_file = \"path/to/your/audiofile.wav\"\n",
    "activity = detect_voice_activity(audio_file)\n",
    "print(activity)\n",
    "```\n",
    "\n",
    "3. Using speech_recognition\n",
    "```python\n",
    "import speech_recognition as sr\n",
    "\n",
    "def detect_voice_activity(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio = recognizer.record(source)\\\n",
    "        try:\n",
    "            recognizer.recognize_google(audio)\n",
    "            return True # Voice activity detected\n",
    "        except sr.UnknownValueError:\n",
    "            return False # No voice activity detected\n",
    "\n",
    "audio_file = \"path/to/your/audiofile.wav\"\n",
    "activity = detect_voice_activity(audio_file)\n",
    "print(activity)\n",
    "```\n",
    "These methods provide different ways to detect voice activity, each with its own strengths and use cases. Choose the one that best fits your needs and the specifics of your project.\n",
    "\n",
    "\n",
    "Corresponding github repos (used): https://github.com/MorenoLaQuatra/vad/tree/main\n",
    "\n",
    "Corresponding github repos (explored): https://github.com/Picovoice/cobra/tree/main, https://github.com/wiseman/py-webrtcvad/tree/master, https://github.com/NickWilkinson37/voxseg/tree/master\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
